{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9435890,"sourceType":"datasetVersion","datasetId":5733347}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-21T16:31:42.247145Z","iopub.execute_input":"2024-09-21T16:31:42.247418Z","iopub.status.idle":"2024-09-21T16:31:42.627379Z","shell.execute_reply.started":"2024-09-21T16:31:42.247375Z","shell.execute_reply":"2024-09-21T16:31:42.626440Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/link-da/2.-Harry-Potter-and-the-Chamber-of-Secrets.pdf\n/kaggle/input/link-da/4.-Harry-Potter-and-the-Goblet.pdf\n/kaggle/input/link-da/1.-Harry-Potter-and-the-Philosophers-Stone.pdf\n/kaggle/input/link-da/3.-Harry-Potter-and-the-Prisoner-of-Azkaban.pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:31:42.629380Z","iopub.execute_input":"2024-09-21T16:31:42.630104Z","iopub.status.idle":"2024-09-21T16:31:57.098802Z","shell.execute_reply.started":"2024-09-21T16:31:42.630069Z","shell.execute_reply":"2024-09-21T16:31:57.097712Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:31:57.100418Z","iopub.execute_input":"2024-09-21T16:31:57.100832Z","iopub.status.idle":"2024-09-21T16:32:13.810985Z","shell.execute_reply.started":"2024-09-21T16:31:57.100786Z","shell.execute_reply":"2024-09-21T16:32:13.809852Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.5.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:13.813458Z","iopub.execute_input":"2024-09-21T16:32:13.813791Z","iopub.status.idle":"2024-09-21T16:32:14.261967Z","shell.execute_reply.started":"2024-09-21T16:32:13.813757Z","shell.execute_reply":"2024-09-21T16:32:14.261020Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1.5.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 1b: Implement code to build the Term-document Matrix","metadata":{}},{"cell_type":"code","source":"import PyPDF2\n\n#Extracting\ndef extract_text_from_pdf(pdf_file):\n    text = \"\"\n    with open(pdf_file, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)  # Use PdfReader instead of PdfFileReader\n        for page_num in range(len(reader.pages)):  # reader.pages gives a list of pages\n            page = reader.pages[page_num]\n            text += page.extract_text()\n    return text\n\nbook1 = extract_text_from_pdf(\"/kaggle/input/link-da/1.-Harry-Potter-and-the-Philosophers-Stone.pdf\")\nbook2 = extract_text_from_pdf(\"/kaggle/input/link-da/2.-Harry-Potter-and-the-Chamber-of-Secrets.pdf\")\nbook3 = extract_text_from_pdf(\"/kaggle/input/link-da/3.-Harry-Potter-and-the-Prisoner-of-Azkaban.pdf\")\nbook4 = extract_text_from_pdf(\"/kaggle/input/link-da/4.-Harry-Potter-and-the-Goblet.pdf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:14.263140Z","iopub.execute_input":"2024-09-21T16:32:14.263642Z","iopub.status.idle":"2024-09-21T16:32:38.050517Z","shell.execute_reply.started":"2024-09-21T16:32:14.263607Z","shell.execute_reply":"2024-09-21T16:32:38.049581Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Text Preprocessing:\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n    return tokens\n\nprocessed_text1 = preprocess_text(book1)\nprocessed_text2 = preprocess_text(book2)\nprocessed_text3 = preprocess_text(book3)\nprocessed_text4 = preprocess_text(book4)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:38.051679Z","iopub.execute_input":"2024-09-21T16:32:38.052091Z","iopub.status.idle":"2024-09-21T16:32:45.398722Z","shell.execute_reply.started":"2024-09-21T16:32:38.052046Z","shell.execute_reply":"2024-09-21T16:32:45.397912Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef ensure_string(text):\n    if isinstance(text, list):\n        return ' '.join(text)  \n\ndef build_term_document_matrix(documents):\n    vectorizer = CountVectorizer() \n    X = vectorizer.fit_transform(documents)  \n    return X, vectorizer.get_feature_names_out()  \n\ndocuments = [\n    ensure_string(processed_text1),\n    ensure_string(processed_text2),\n    ensure_string(processed_text3),\n    ensure_string(processed_text4)\n]\n\nstart_time = time.time()\n\nterm_document_matrix, terms = build_term_document_matrix(documents)\n\nexecution_time = time.time() - start_time\n\n\nprint(\"Term-Document Matrix (Raw Frequency):\")\nprint(term_document_matrix.toarray()) \n\nprint(\"\\nTerms (Features):\")\nprint(terms)  \n\nprint(f\"\\nExecution Time: {execution_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:45.399839Z","iopub.execute_input":"2024-09-21T16:32:45.400252Z","iopub.status.idle":"2024-09-21T16:32:45.689709Z","shell.execute_reply.started":"2024-09-21T16:32:45.400220Z","shell.execute_reply":"2024-09-21T16:32:45.688733Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Term-Document Matrix (Raw Frequency):\n[[ 1  1  1 ...  2  0  1]\n [ 1  1  1 ...  0  0  0]\n [ 1  1  1 ...  3  0  0]\n [ 1  1  1 ... 11  1  3]]\n\nTerms (Features):\n['10' '100' '101' ... 'zooming' 'éclair' 'éclairs']\n\nExecution Time: 0.26993680000305176 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 1c: Compute Similarities Using Euclidean Distance and Cosine Similarity","metadata":{}},{"cell_type":"code","source":"# Euclidean Distance\nfrom sklearn.metrics.pairwise import euclidean_distances\n\neuclidean_dist = euclidean_distances(term_document_matrix)\nprint(\"Euclidean Distance Matrix:\\n\", euclidean_dist)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:45.690820Z","iopub.execute_input":"2024-09-21T16:32:45.691134Z","iopub.status.idle":"2024-09-21T16:32:45.702180Z","shell.execute_reply.started":"2024-09-21T16:32:45.691101Z","shell.execute_reply":"2024-09-21T16:32:45.701285Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Euclidean Distance Matrix:\n [[   0.          916.1129843  1463.21256146 3420.60564813]\n [ 916.1129843     0.         1134.279507   2952.26082859]\n [1463.21256146 1134.279507      0.         2456.55042692]\n [3420.60564813 2952.26082859 2456.55042692    0.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Euclidean distance shows how far apart the books are in the vector space. Lower values indicate more similarity.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim = cosine_similarity(term_document_matrix)\nprint(\"Cosine Similarity Matrix:\\n\", cosine_sim)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:45.703332Z","iopub.execute_input":"2024-09-21T16:32:45.703915Z","iopub.status.idle":"2024-09-21T16:32:45.714295Z","shell.execute_reply.started":"2024-09-21T16:32:45.703853Z","shell.execute_reply":"2024-09-21T16:32:45.713437Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cosine Similarity Matrix:\n [[1.         0.94329679 0.9416178  0.93467331]\n [0.94329679 1.         0.95078986 0.95214857]\n [0.9416178  0.95078986 1.         0.94577085]\n [0.93467331 0.95214857 0.94577085 1.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Cosine similarity will indicate how similar the books are based on the angle between their term vectors. Values closer to 1 indicate more similarity.","metadata":{}},{"cell_type":"markdown","source":"# Task 1d: Select representative words and compute reduced embedding.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\n\ndocuments = [' '.join(processed_text1), \n             ' '.join(processed_text2), \n             ' '.join(processed_text3), \n             ' '.join(processed_text4)]\n\nrepresentative_words = ['wizard', 'magical', 'harry', 'spells', 'witch', 'journey']\n\n# Create a CountVectorizer with the selected words\nvectorizer = CountVectorizer(vocabulary = representative_words)\nterm_document_matrix = vectorizer.fit_transform(documents).toarray()\n\ndf = pd.DataFrame(term_document_matrix, columns = representative_words)\nprint(\"Term-Document Matrix for Selected Words:\\n\", df)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:45.716739Z","iopub.execute_input":"2024-09-21T16:32:45.717046Z","iopub.status.idle":"2024-09-21T16:32:45.974781Z","shell.execute_reply.started":"2024-09-21T16:32:45.717014Z","shell.execute_reply":"2024-09-21T16:32:45.973853Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Term-Document Matrix for Selected Words:\n    wizard  magical  harry  spells  witch  journey\n0      42       11   1308       6     12       11\n1      47       16   1633       9     16        5\n2      39       31   2035       2     42        6\n3      83      125   3134      24     37       10\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compute cosine similarity\ncosine_sim = cosine_similarity(term_document_matrix)\nprint(\"Cosine Similarity Matrix for Selected Words:\\n\", cosine_sim)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T16:32:45.976003Z","iopub.execute_input":"2024-09-21T16:32:45.976405Z","iopub.status.idle":"2024-09-21T16:32:45.983590Z","shell.execute_reply.started":"2024-09-21T16:32:45.976357Z","shell.execute_reply":"2024-09-21T16:32:45.982621Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Cosine Similarity Matrix for Selected Words:\n [[1.         0.99997861 0.99980603 0.99946822]\n [0.99997861 1.         0.9998701  0.99954149]\n [0.99980603 0.9998701  1.         0.999609  ]\n [0.99946822 0.99954149 0.999609   1.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"By focusing on key words related to the Harry Potter universe, the similarity scores will increase, as the key words will represent the core themes of each book more accurately than the full vocabulary.","metadata":{}},{"cell_type":"code","source":"# Reduce dimensionality using Truncated SVD\nn_components = 2\nsvd = TruncatedSVD(n_components=n_components)\nreduced_embeddings = svd.fit_transform(term_document_matrix)\n\nprint(\"Reduced Embeddings:\\n\", reduced_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T17:23:35.530793Z","iopub.execute_input":"2024-09-21T17:23:35.531189Z","iopub.status.idle":"2024-09-21T17:23:35.553625Z","shell.execute_reply.started":"2024-09-21T17:23:35.531152Z","shell.execute_reply":"2024-09-21T17:23:35.552674Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Reduced Embeddings:\n [[1308.55565611  -23.64973942]\n [1633.60579063  -26.99254366]\n [2035.79150708  -26.59805943]\n [3137.64510112   41.17428047]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 7: Derive the formula for Conditional Entropy","metadata":{}},{"cell_type":"markdown","source":"## Entropy\n\nThe entropy \\( H(X) \\) of a discrete random variable \\( X \\) with probability mass function \\( p(x) \\) is defined as:\n\n$$\nH(X) = - \\sum_{x} p(x) \\log p(x)\n$$\n\nThis measures the average amount of uncertainty or information contained in the random variable \\( X \\).\n\n## Joint Entropy\n\nThe joint entropy \\( H(X, Y) \\) of two discrete random variables \\( X \\) and \\( Y \\) with joint probability mass function \\( p(x, y) \\) is defined as:\n\n$$\nH(X, Y) = - \\sum_{x, y} p(x, y) \\log p(x, y)\n$$\n\nThis measures the uncertainty or information contained in the pair \\( (X, Y) \\).\n\n## Conditional Probability\n\nThe conditional probability \\( p(x|y) \\) of \\( X \\) given \\( Y \\) is defined as:\n\n$$\np(x|y) = \\frac{p(x, y)}{p(y)}\n$$\n\nThis represents the probability of \\( X \\) occurring given that \\( Y \\) has occurred.\n\n## Conditional Entropy\n\nThe conditional entropy \\( H(X|Y) \\) quantifies the amount of information needed to describe the outcome of \\( X \\) given that the value of \\( Y \\) is known. It is defined as:\n\n$$\nH(X|Y) = - \\sum_{x, y} p(x, y) \\log p(x|y)\n$$\n\nSubstituting the definition of conditional probability \\( p(x|y) \\):\n\n$$\nH(X|Y) = - \\sum_{x, y} p(x, y) \\log \\left( \\frac{p(x, y)}{p(y)} \\right)\n$$\n\nUsing the properties of logarithms, we can separate the terms inside the logarithm:\n\n$$\nH(X|Y) = - \\sum_{x, y} p(x, y) \\left[ \\log p(x, y) - \\log p(y) \\right]\n$$\n\nDistributing \\( p(x, y) \\) and summing over all \\( x \\) and \\( y \\):\n\n$$\nH(X|Y) = - \\sum_{x, y} p(x, y) \\log p(x, y) + \\sum_{x, y} p(x, y) \\log p(y)\n$$\n\nNotice that the second term simplifies because $$ \\sum_{x} p(x, y) = p(y) $$\n\n$$\nH(X|Y) = H(X, Y) - H(Y)\n$$\n\nThus, the conditional entropy \\( H(X|Y) \\) is the difference between the joint entropy \\( H(X, Y) \\) and the entropy \\( H(Y) \\). This makes sense intuitively, as it measures the remaining uncertainty in X after taking into account the information provided by Y.\n\nThe definition involves the ratio \\( p(x, y) / p(y) \\) because it represents the conditional probability ( p(x|y) ), which is crucial for understanding how the knowledge of ( Y ) affects the uncertainty of ( X ).\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}