{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:31:42.630104Z","iopub.status.busy":"2024-09-21T16:31:42.629380Z","iopub.status.idle":"2024-09-21T16:31:57.098802Z","shell.execute_reply":"2024-09-21T16:31:57.097712Z","shell.execute_reply.started":"2024-09-21T16:31:42.630069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting PyPDF2Note: you may need to restart the kernel to use updated packages.\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\n","Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\admin-pc\\anaconda3\\lib\\site-packages (from PyPDF2) (4.8.0)\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}],"source":["pip install PyPDF2"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:31:57.100832Z","iopub.status.busy":"2024-09-21T16:31:57.100418Z","iopub.status.idle":"2024-09-21T16:32:13.810985Z","shell.execute_reply":"2024-09-21T16:32:13.809852Z","shell.execute_reply.started":"2024-09-21T16:31:57.100786Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scikit-learn in c:\\users\\admin-pc\\anaconda3\\lib\\site-packages (1.0.2)"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n","streamlit 1.14.0 requires protobuf<4,>=3.12, but you have protobuf 4.23.4 which is incompatible.\n","numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.22.4 which is incompatible.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.5.2-cp39-cp39-win_amd64.whl (11.0 MB)\n","Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin-pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin-pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n","Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin-pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.3)\n","Collecting threadpoolctl>=3.1.0\n","  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n","Collecting numpy>=1.19.5\n","  Using cached numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n","Installing collected packages: numpy, threadpoolctl, scikit-learn\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.3\n","    Uninstalling numpy-1.26.3:\n","      Successfully uninstalled numpy-1.26.3\n","  Attempting uninstall: threadpoolctl\n","    Found existing installation: threadpoolctl 2.2.0\n","    Uninstalling threadpoolctl-2.2.0:\n","      Successfully uninstalled threadpoolctl-2.2.0\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","Successfully installed numpy-1.22.4 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"]}],"source":["!pip install --upgrade scikit-learn"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:13.813791Z","iopub.status.busy":"2024-09-21T16:32:13.813458Z","iopub.status.idle":"2024-09-21T16:32:14.261967Z","shell.execute_reply":"2024-09-21T16:32:14.261020Z","shell.execute_reply.started":"2024-09-21T16:32:13.813757Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.5.2\n"]}],"source":["import sklearn\n","print(sklearn.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Task 1b: Implement code to build the Term-document Matrix"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:14.263642Z","iopub.status.busy":"2024-09-21T16:32:14.263140Z","iopub.status.idle":"2024-09-21T16:32:38.050517Z","shell.execute_reply":"2024-09-21T16:32:38.049581Z","shell.execute_reply.started":"2024-09-21T16:32:14.263607Z"},"trusted":true},"outputs":[],"source":["import PyPDF2\n","\n","#Extracting\n","def extract_text_from_pdf(pdf_file):\n","    text = \"\"\n","    with open(pdf_file, 'rb') as file:\n","        reader = PyPDF2.PdfReader(file)  # Use PdfReader instead of PdfFileReader\n","        for page_num in range(len(reader.pages)):  # reader.pages gives a list of pages\n","            page = reader.pages[page_num]\n","            text += page.extract_text()\n","    return text\n","\n","book1 = extract_text_from_pdf(\"C:/Users/ADMIN-PC/Downloads/1.-Harry-Potter-and-the-Philosophers-Stone.pdf\")\n","book2 = extract_text_from_pdf(\"C:/Users/ADMIN-PC/Downloads/2.-Harry-Potter-and-the-Chamber-of-Secrets.pdf\")\n","book3 = extract_text_from_pdf(\"C:/Users/ADMIN-PC/Downloads/3.-Harry-Potter-and-the-Prisoner-of-Azkaban.pdf\")\n","book4 = extract_text_from_pdf(\"C:/Users/ADMIN-PC/Downloads/4.-Harry-Potter-and-the-Goblet.pdf\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:38.052091Z","iopub.status.busy":"2024-09-21T16:32:38.051679Z","iopub.status.idle":"2024-09-21T16:32:45.398722Z","shell.execute_reply":"2024-09-21T16:32:45.397912Z","shell.execute_reply.started":"2024-09-21T16:32:38.052046Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\ADMIN-\n","[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n","[nltk_data] Downloading package stopwords to C:\\Users\\ADMIN-\n","[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Text Preprocessing:\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return tokens\n","\n","processed_text1 = preprocess_text(book1)\n","processed_text2 = preprocess_text(book2)\n","processed_text3 = preprocess_text(book3)\n","processed_text4 = preprocess_text(book4)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:45.400252Z","iopub.status.busy":"2024-09-21T16:32:45.399839Z","iopub.status.idle":"2024-09-21T16:32:45.689709Z","shell.execute_reply":"2024-09-21T16:32:45.688733Z","shell.execute_reply.started":"2024-09-21T16:32:45.400220Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Term-Document Matrix (Raw Frequency):\n","[[ 1  1  1 ...  2  0  1]\n"," [ 1  1  1 ...  0  0  0]\n"," [ 1  1  1 ...  3  0  0]\n"," [ 1  1  1 ... 11  1  3]]\n","\n","Terms (Features):\n","['10' '100' '101' ... 'zooming' 'éclair' 'éclairs']\n","\n","Execution Time: 0.27508544921875 seconds\n"]}],"source":["import time\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def ensure_string(text):\n","    if isinstance(text, list):\n","        return ' '.join(text)  \n","\n","def build_term_document_matrix(documents):\n","    vectorizer = CountVectorizer() \n","    X = vectorizer.fit_transform(documents)  \n","    return X, vectorizer.get_feature_names_out()  \n","\n","documents = [\n","    ensure_string(processed_text1),\n","    ensure_string(processed_text2),\n","    ensure_string(processed_text3),\n","    ensure_string(processed_text4)\n","]\n","\n","start_time = time.time()\n","\n","term_document_matrix, terms = build_term_document_matrix(documents)\n","\n","execution_time = time.time() - start_time\n","\n","\n","print(\"Term-Document Matrix (Raw Frequency):\")\n","print(term_document_matrix.toarray()) \n","\n","print(\"\\nTerms (Features):\")\n","print(terms)  \n","\n","print(f\"\\nExecution Time: {execution_time} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["# Task 1c: Compute Similarities Using Euclidean Distance and Cosine Similarity"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:45.703915Z","iopub.status.busy":"2024-09-21T16:32:45.703332Z","iopub.status.idle":"2024-09-21T16:32:45.714295Z","shell.execute_reply":"2024-09-21T16:32:45.713437Z","shell.execute_reply.started":"2024-09-21T16:32:45.703853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine Similarity Matrix:\n"," [[1.         0.94329679 0.9416178  0.93467331]\n"," [0.94329679 1.         0.95078986 0.95214857]\n"," [0.9416178  0.95078986 1.         0.94577085]\n"," [0.93467331 0.95214857 0.94577085 1.        ]]\n"]}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","cosine_sim = cosine_similarity(term_document_matrix)\n","print(\"Cosine Similarity Matrix:\\n\", cosine_sim)\n"]},{"cell_type":"markdown","metadata":{},"source":["Cosine similarity will indicate how similar the books are based on the angle between their term vectors. Values closer to 1 indicate more similarity."]},{"cell_type":"markdown","metadata":{},"source":["# Task 1d: Select representative words and compute reduced embedding."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T16:32:45.717046Z","iopub.status.busy":"2024-09-21T16:32:45.716739Z","iopub.status.idle":"2024-09-21T16:32:45.974781Z","shell.execute_reply":"2024-09-21T16:32:45.973853Z","shell.execute_reply.started":"2024-09-21T16:32:45.717014Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ADMIN-PC\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1364: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Term-Document Matrix for selected representative words:\n","   Harry  Hogwarts  magic  wand  Voldemort  wizard  spell  Ron  Slytherin\n","0      0         0     46    60          0      42     14    0          0\n","1      0         0     52   105          0      47      6    0          0\n","2      0         0     60   113          0      39      6    0          0\n","3      0         0     85   272          0      83     35    0          0\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import TruncatedSVD\n","\n","# Combine the processed texts into a list\n","documents1 = [' '.join(processed_text1), \n","             ' '.join(processed_text2), \n","             ' '.join(processed_text3), \n","             ' '.join(processed_text4)]\n","\n","\n","representative_words = [\n","    'Harry', 'Hogwarts', 'magic', 'wand', \n","    'Voldemort', 'wizard', 'spell', 'Ron', 'Slytherin'\n","]\n","\n","def build_selected_term_document_matrix(documents, selected_words):\n","    vectorizer = CountVectorizer(vocabulary=selected_words)\n","    X = vectorizer.fit_transform(documents)\n","    return X, vectorizer.get_feature_names_out()\n","\n","selected_term_document_matrix, selected_terms = build_selected_term_document_matrix(documents, representative_words)\n","\n","print(\"Term-Document Matrix for selected representative words:\")\n","print(pd.DataFrame(selected_term_document_matrix.toarray(), columns=selected_terms))\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine Similarity Matrix:\n","           Book 1    Book 2    Book 3    Book 4\n","Book 1  1.000000  0.970810  0.960078  0.924430\n","Book 2  0.970810  1.000000  0.995989  0.981929\n","Book 3  0.960078  0.995989  1.000000  0.981706\n","Book 4  0.924430  0.981929  0.981706  1.000000\n"]}],"source":["cosine_sim = cosine_similarity(selected_term_document_matrix)\n","print(\"Cosine Similarity Matrix:\\n\", pd.DataFrame(cosine_sim, columns=[\"Book 1\", \"Book 2\", \"Book 3\", \"Book 4\"], index=[\"Book 1\", \"Book 2\", \"Book 3\", \"Book 4\"]))"]},{"cell_type":"markdown","metadata":{},"source":["By focusing on key words related to the Harry Potter universe, overall the similarity scores will increase, as the key words will represent the core themes of each book more accurately than the full vocabulary."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TruncatedSVD Reduced Embedding:\n","   Component 1  Component 2\n","0    82.839655    26.913382\n","1   125.388551    14.723218\n","2   132.735560    13.444964\n","3   298.203681   -19.651813\n"]}],"source":["from sklearn.decomposition import TruncatedSVD\n","import pandas as pd\n","\n","def compute_reduced_embedding(term_document_matrix, n_components=2):\n","    svd = TruncatedSVD(n_components=n_components)\n","    reduced_matrix = svd.fit_transform(term_document_matrix)\n","    return reduced_matrix\n","\n","reduced_embedding = compute_reduced_embedding(selected_term_document_matrix)\n","\n","print(\"TruncatedSVD Reduced Embedding:\")\n","print(pd.DataFrame(reduced_embedding, columns=[f'Component {i+1}' for i in range(reduced_embedding.shape[1])]))\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:23:35.531189Z","iopub.status.busy":"2024-09-21T17:23:35.530793Z","iopub.status.idle":"2024-09-21T17:23:35.553625Z","shell.execute_reply":"2024-09-21T17:23:35.552674Z","shell.execute_reply.started":"2024-09-21T17:23:35.531152Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PCA Reduced Embedding:\n","   Component 1  Component 2\n","0   -78.974195    10.024436\n","1   -34.890374    -2.421648\n","2   -27.466978   -10.555147\n","3   141.331546     2.952359\n"]}],"source":["from sklearn.decomposition import PCA\n","\n","# Step 5: Compute the reduced embedding using PCA\n","def compute_pca_embedding(term_document_matrix, n_components=2):\n","    pca = PCA(n_components=n_components)\n","    pca_reduced_matrix = pca.fit_transform(term_document_matrix.toarray())\n","    return pca_reduced_matrix\n","\n","# Compute the PCA reduced embedding\n","pca_reduced_embedding = compute_pca_embedding(selected_term_document_matrix)\n","\n","# Display the PCA reduced embedding\n","print(\"PCA Reduced Embedding:\")\n","print(pd.DataFrame(pca_reduced_embedding, columns=[f'Component {i+1}' for i in range(pca_reduced_embedding.shape[1])]))\n"]},{"cell_type":"markdown","metadata":{},"source":["I think Truncated SVD is a more preferred choice in NLP than PCA because Truncated SVD is specifically designed to work efficiently with large data sets and sparse matrices, just like document term matrices, and no need for data centering too, while PCA can struggle with them."]},{"cell_type":"markdown","metadata":{},"source":["# Task 7: Derive the formula for Conditional Entropy"]},{"cell_type":"markdown","metadata":{},"source":["pls give me more time :("]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5733347,"sourceId":9435890,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
